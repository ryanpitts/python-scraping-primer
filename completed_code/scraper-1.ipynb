{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping from a live site\n",
    "\n",
    "In our [starter exercise](scraper-0.ipynb), we stepped through the basics of writing a scraper, picking up some principles we'll use again and again.\n",
    "\n",
    "* Inspect the page to find the HTML tags wrapping the data you care about\n",
    "* Use Python to open the file\n",
    "* Parse that HTML and loop through it, storing the data you want\n",
    "* Save it into a CSV for analysis later\n",
    "\n",
    "We also started by scraping a really simple file, but scraping a more complex page pretty much follows these same steps. This time we'll need to get a little more specific as we target our data.\n",
    "\n",
    "And the file in our first example was a copy we had saved locallyâ€”which is a great approach when you can use it! But sometimes you might want to scrape directly from a live page on the internet, like if a page updates regularly and you want to automate your scraper to fetch new data each day.\n",
    "\n",
    "That's the approach we'll take here, to scrape [a list of court cases](https://cp.spokanecounty.org/courtdocumentviewer/PublicViewer/SCHearingsByDate.aspx?d=01/23/2019). We'll start our Python script the same as last time, importing the `BeautifulSoup` and `csv` libraries. And we'll add a new library called `requests` to help us grab our page from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Python libraries we need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case our wifi fails, we have a local copy of the page we can work with. But we'll try to just leave the `open()` method we used last time commented out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we're working from a local copy of our page ...\n",
    "# use Python's open() to open the HTML page\n",
    "# html = open('pages/scraper-1-page.html', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests` library is amazingly handy, and we'll barely be scratching the surface of what it can do. To get our page from the internet, we'll define a couple variables first: `URL` to hold our page's web address, and `HEADERS` to share a bit of information in their server logs. Then we can use `requests` to `get()` our page, and pull its contents into a variable we can work with.\n",
    "\n",
    "(We'll be hard-coding the date in our `URL` variable ... for now. But let's get our scraper working first before we think about how to make it more flexible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if we're requesting a live page over the internet ...\n",
    "# define the URL we want to scrape\n",
    "URL = 'https://cp.spokanecounty.org/courtdocumentviewer/PublicViewer/SCHearingsByDate.aspx?d=01/23/2019'\n",
    "\n",
    "# define the headers our scraper will pass, so we look like a browser\n",
    "# https://developers.whatismybrowser.com/useragents/explore/\n",
    "HEADERS = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0',}\n",
    "\n",
    "# use requests to fetch that URL\n",
    "page = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "# and store the page content in a Python variable\n",
    "html = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're back into familiar territory, using `BeautifulSoup` to parse our HTML and making an empty list to hold our row data. We're going to be a little more specific this time when we tell `BeautifulSoup` where our data is, though! Just in case there's more than one `<table>` on the page, we'll identify the one we want by its `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however we opened it, we have our page so use BeautifulSoup to parse it\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# make ourselves an empty list to hold data for a CSV\n",
    "list_of_rows = []\n",
    "\n",
    "# use BeautifulSoup to find the table in our parsed HTML\n",
    "table = soup.find(id='tblHearingsSCByDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also be a little more specific here when we loop through our table rows. The `<tr>` elements with the data we want all have a CSS class of `detailrow`, so let's specify those and ignore any other rows we run into.\n",
    "\n",
    "Another problem we're going to run into: One of our `<td>` cells has some duplicated information in it. But luckily it's wrapped in a `<span>` tag, so we can use `BeautifulSoup` to `clear()` it right out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the rows in our table using BeautifulSoup\n",
    "for row in table.find_all('tr', class_='detailrow'):\n",
    "    # create an empty list each time through, to hold cell data\n",
    "    list_of_cells = []\n",
    "    # loop through each cell in this table row\n",
    "    for cell in row.find_all('td'):\n",
    "        # we noticed some cruft on one cell, so get rid of it\n",
    "        if cell.span:\n",
    "            cell.span.clear()\n",
    "        # grab the text from that cell\n",
    "        text = cell.text.strip()\n",
    "        # and append it to our list\n",
    "        list_of_cells.append(text)\n",
    "    # when we're done with this table row, append its data to our list of rows\n",
    "    list_of_rows.append(list_of_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've looped through our table, ignored rows we don't care about and some duplicated data, and now we're ready to write everything into a CSV. The only thing to change about this from last time is our filename for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Python's CSV library to create our output file\n",
    "outfile = open('docket.csv', 'w', newline='', encoding='utf-8')\n",
    "writer = csv.writer(outfile)\n",
    "writer.writerows(list_of_rows)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have scraped yet again, hopefully from all the way across the internet.\n",
    "\n",
    "That's awesome, but let's take a second to think about how we could make this scraper even better. That URL we're scraping has a date parameter ... and that sounds like an invitation to make our script flexible enough to grab data day after day without having to edit things by hand.\n",
    "\n",
    "To do that, we'd revisit the top of our scraper, importing Python's built-in `date` library and creating a variable that holds today's date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could use our `today` variable to change the URL we want to scrape ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cp.spokanecounty.org/courtdocumentviewer/PublicViewer/SCHearingsByDate.aspx?d=08/21/2019\n"
     ]
    }
   ],
   "source": [
    "url_date = today.strftime('%m/%d/%Y')\n",
    "URL = 'https://cp.spokanecounty.org/courtdocumentviewer/PublicViewer/SCHearingsByDate.aspx?d={}'.format(url_date)\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could use it to name our output file in a way that keeps everything organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docket-2019-08-21.csv\n"
     ]
    }
   ],
   "source": [
    "filename_date = today.strftime('%Y-%m-%d')\n",
    "outfile_filename = 'docket-{}.csv'.format(filename_date)\n",
    "print(outfile_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Just a couple small tweaks and this scraper is ready for automation. There are plenty of other things we could add, too.\n",
    "\n",
    "* We could email a reporter each time the scraper runs, attaching the daily data file.\n",
    "* Once we've populated `list_of_rows`, we can use it as much as we want. We could open a second CSV in append mode, and add the daily data to it for a longer-term analysis.\n",
    "* We could make ourselves a list of `ALERT_NAMES` and check for matches each time the scraper runs.\n",
    "\n",
    "We're just writing Python here, so we're only limited by our imagination and our ability to search for answers we can copy/paste from StackOverflow.\n",
    "\n",
    "Our [final exercise](scraper-2.ipynb) adds one more approach to our scraping toolkit. Sometimes the dataset we want is broken up across multiple web pages, but we won't let that throw us for a `loop`. (Yeahhhhh, I feel pretty guilty about typing that.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

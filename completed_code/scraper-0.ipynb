{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping basics\n",
    "\n",
    "There are a few steps you'll follow pretty much every time you write a new web scraper:\n",
    "\n",
    "1. Inspect the underlying HTML on the page you want to scrape. Use \"View Source\" in your browser, or right-click and \"Inspect Element\" to take a look at how the HTML tags are structured around the data you want to capture.\n",
    "2. In the programming language of your choice (we're using Python here), write a script that:\n",
    "    * Opens that page from the internet\n",
    "    * Parses its HTML, using the tags you spotted earlier as a guide\n",
    "    * Saves that data for later\n",
    "3. Test your scraper, fix what's broken, and run it again till it works.\n",
    "\n",
    "Feels like a pretty straightforward process, but the code underneath the pages where your data lives are often _anything_ but straightforward. So let's start by stepping through the process with a super simple example.\n",
    "\n",
    "[This page just has one table on it](pages/scraper-0-page-example-table.html), holding some recent data about NICAR conference locations. Try \"View Source\" and take a look at the code underneath. A `<table>` tag, some `<tr>` table rows inside of it, and some `<td>` table cells inside those rows. Just what we like to see.\n",
    "\n",
    "So let's get that data out of there! In the script we'll build below, our `#` code comments will guide us through each piece of code to write. The first thing we need to do in our Python script is import the libraries we'll be using here: `BeautifulSoup` for parsing HTML and `csv` to write our data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Python libraries we need\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load our page and parse it. Our local copy of the HTML makes that easy, and we'll use `BeautifulSoup` to turn it into a Python object we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Python's open() to open the HTML page we've stored locally\n",
    "page = open('pages/scraper-0-page-example-table.html', 'r')\n",
    "\n",
    "# use BeautifulSoup to parse that page into Python\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "# and close the HTML page\n",
    "page.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we want to make ourselves a CSV later on, so we'll need an empty Python list to start stuffing each row of data into. And we can use `BeautifulSoup` again to find just the part of our page we care about: the `<table>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ourselves an empty list to hold data for a CSV\n",
    "list_of_rows = []\n",
    "\n",
    "# use BeautifulSoup to find the table in our parsed HTML\n",
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to start extracting data! We need a Python loop that:\n",
    "\n",
    "* goes through each `<tr>` in our `<table>`\n",
    "* creates a new, temporary list to hold the cell data it contains\n",
    "* loops through each `<td>` in that row, adding its text to our temporary list\n",
    "* and once we've processed the full row, append it all to our master list of row data.\n",
    "\n",
    "Then our loop will move on to the next row, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the rows in our table using BeautifulSoup\n",
    "for row in table.find_all('tr'):\n",
    "    # create an empty list each time through, to hold cell data\n",
    "    list_of_cells = []\n",
    "    # loop through each cell in this table row\n",
    "    for cell in row.find_all('td'):\n",
    "        # grab the text from that cell\n",
    "        text = cell.text.strip()\n",
    "        # and append it to our list\n",
    "        list_of_cells.append(text)\n",
    "    # when we're done with this table row, append its data to our list of rows\n",
    "    list_of_rows.append(list_of_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data successfully extracted and stored in a nice, big list of lists, we can use Pyton's built-in `csv` library to write ourselves a CSV to analyze later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Python's CSV library to create our output file\n",
    "outfile = open('nicar_cities.csv', 'w')\n",
    "writer = csv.writer(outfile)\n",
    "writer.writerows(list_of_rows)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, we scraped!\n",
    "\n",
    "Our [next exercise](scraper-1.ipynb) adds just a bit of complexity. We'll scrape a page with more than just a `<table>` on it, and we'll pull it straight off the internet (wifi willing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
